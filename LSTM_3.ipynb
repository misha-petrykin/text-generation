{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gensim\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import get_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
    "path = get_file('arxiv_abstracts.txt', origin=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = 40\n",
    "with open(path) as file_:\n",
    "    docs = file_.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n",
      "1108584\n",
      "3589\n"
     ]
    }
   ],
   "source": [
    "count_words=0\n",
    "unique_words=list()\n",
    "for i in docs:\n",
    "    count_words+=len(i.split())\n",
    "    unique_words.extend(i.split())\n",
    "print(len(docs))\n",
    "print(count_words)\n",
    "print(len(set(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 7200\n"
     ]
    }
   ],
   "source": [
    "sentences = [[word for word in doc.lower().translate(string.punctuation).split()[:max_sentence_len]] for doc in docs]\n",
    "print('Num sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec...\n",
      "Result embedding shape: (1351, 100)\n",
      "Checking similar words:\n",
      "  model -> $l_p$ (0.40), architecture. (0.34), of (0.34), trains (0.32), estimate (0.29), continuous (0.29), 2012) (0.29), designing (0.29)\n",
      "  network -> networks (0.43), constrained (0.29), networks. (0.28), connected (0.25), architecture (0.24), cluster (0.23), algorithm (0.23), fully (0.22)\n",
      "  train -> based (0.38), eigendecompositions (0.35), construct (0.33), tend (0.33), then (0.32), directly (0.32), classical (0.31), derive (0.29)\n",
      "  learn -> tend (0.38), 98.8%. (0.37), effort (0.37), adopted (0.36), automatically (0.36), effectively (0.35), remain (0.35), adapt (0.35)\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining word2vec...')\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.vectors\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['model', 'network', 'train', 'learn']:\n",
    "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n",
    "    print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "science\n",
      "and\n",
      "engineering,\n",
      "intelligent\n",
      "processing\n",
      "of\n",
      "complex\n",
      "signals\n",
      "such\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.0546215e+00, -7.7986413e-01,  3.2261524e+00, -3.0187001e+00,\n",
       "        3.4604156e+00, -1.0047338e+00,  7.7713805e-01, -2.9266636e+00,\n",
       "        9.3600464e-01, -3.3955967e+00, -1.7219131e+00, -1.0482934e+00,\n",
       "       -5.1704621e+00,  1.3606992e+00, -1.2728568e+00, -3.6279399e+00,\n",
       "        4.8442054e-01,  1.1889240e+00, -1.1569999e+00,  2.3931613e+00,\n",
       "       -5.3412516e-02, -2.7056301e-02, -3.5397619e-01, -5.5631888e-01,\n",
       "        3.4001482e+00,  3.9717436e-01, -1.5980237e+00,  1.8993194e+00,\n",
       "        8.9297110e-01,  2.6208870e-03, -1.6105610e+00,  4.5738953e-01,\n",
       "       -1.5198020e+00, -2.5471356e+00, -5.9460646e-01,  2.0210540e+00,\n",
       "       -1.8610653e+00,  2.1937408e+00, -2.3421328e+00, -1.3236393e+00,\n",
       "        3.4857004e+00,  2.3239241e+00,  2.6127634e+00, -1.5831639e+00,\n",
       "        1.5517576e-01, -4.0484018e+00, -2.6121516e+00,  1.3111370e+00,\n",
       "        4.5106797e+00,  3.6448529e-01,  2.0077325e-01, -5.4873896e-01,\n",
       "       -9.7555661e-01, -5.3487688e-02, -1.0483402e+00, -1.6679634e+00,\n",
       "        2.3457124e+00,  2.9247136e+00,  1.6629971e+00,  1.9384085e+00,\n",
       "       -1.4119679e-01,  9.1382545e-01, -2.0218730e+00, -8.3319038e-01,\n",
       "       -1.7901613e+00,  5.6003851e-01,  3.2147861e+00, -1.4322855e+00,\n",
       "       -2.6199088e-01, -8.6621618e-01, -1.2449598e-01, -4.4746217e-01,\n",
       "        1.0248082e+00, -2.6022630e+00, -3.8558879e-01,  7.3786259e-01,\n",
       "       -1.8408306e-01,  1.4404190e+00, -5.0677255e-02, -1.8951229e+00,\n",
       "       -5.5165666e-01, -1.1516314e+00, -8.3614635e-01, -2.7334387e+00,\n",
       "       -2.5208962e+00, -1.3828410e+00,  2.2742612e+00,  6.5776169e-01,\n",
       "       -3.6242592e+00,  6.9898567e+00, -1.4579806e+00, -1.8176706e+00,\n",
       "        6.7086834e-01,  3.1431613e+00, -6.8062031e-01, -6.5246015e+00,\n",
       "       -1.9786149e+00, -1.3256548e-01,  1.4688028e+00, -1.2577052e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, word in enumerate(word_model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)\n",
    "word_model.wv['in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.19897056e+00,  2.16787471e-03,  8.25989842e-01,  1.40779078e+00,\n",
       "       -1.12605385e-01,  1.49828196e+00,  1.81864396e-01, -7.16304854e-02,\n",
       "        9.91280019e-01, -1.62212014e+00,  2.56801009e-01,  1.64799440e+00,\n",
       "        5.05980730e-01, -1.47530794e+00,  1.38926244e+00,  1.32279098e+00,\n",
       "        1.50041282e+00, -5.81475317e-01, -3.88415933e+00,  1.29780507e+00,\n",
       "        2.32617736e+00, -1.61768007e+00,  8.47442746e-01, -2.09094214e+00,\n",
       "        1.05325782e+00,  1.79176998e+00, -1.32972038e+00,  8.21741760e-01,\n",
       "        1.48245692e+00, -1.18849802e+00,  4.01917362e+00, -5.82825243e-01,\n",
       "       -6.60090685e-01,  1.00047004e+00, -3.27093065e-01,  3.96353036e-01,\n",
       "        5.52804291e-01, -7.86690652e-01, -1.65041220e+00, -1.89783680e+00,\n",
       "       -9.85792756e-01, -1.04100597e+00,  4.52308387e-01, -5.89000463e-01,\n",
       "       -1.34785604e+00, -1.58191609e+00, -2.63793677e-01, -3.19158763e-01,\n",
       "        1.70933568e+00,  3.25072140e-01, -5.65398216e-01, -1.11952996e+00,\n",
       "        2.41511818e-02,  2.73403549e-03,  1.30579889e+00, -7.46762931e-01,\n",
       "        4.23860639e-01,  2.55243331e-01,  1.04743552e+00,  7.86798418e-01,\n",
       "        2.27757525e+00, -1.65150538e-01, -6.54208362e-01,  1.70426095e+00,\n",
       "        2.45105195e+00,  9.28862154e-01,  4.85033363e-01,  2.47111034e+00,\n",
       "       -1.22213614e+00, -4.76818085e-02, -3.28291583e+00, -9.68196392e-01,\n",
       "       -2.15376663e+00, -1.49475884e+00,  9.15535867e-01,  8.51970613e-01,\n",
       "       -1.16161001e+00, -1.82433081e+00,  2.03155041e+00,  1.08837414e+00,\n",
       "       -1.31746459e+00, -9.84658182e-01,  1.37742996e-01, -2.36410189e+00,\n",
       "       -6.57320499e-01,  7.13327885e-01,  2.51262951e+00,  1.73206121e-01,\n",
       "        8.64249527e-01,  1.30993426e+00,  1.43843621e-01, -1.56789303e+00,\n",
       "        4.87963527e-01,  1.80023181e+00, -1.92332804e+00, -1.80249298e+00,\n",
       "       -4.23644543e+00,  1.68311715e+00,  9.25516486e-02, -1.68508911e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]\n"
     ]
    }
   ],
   "source": [
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "# Tokenize the docs\n",
    "tokenized_list = [gensim.utils.simple_preprocess(doc) for doc in my_docs]\n",
    "# Create the Corpus\n",
    "mydict = gensim.corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycorpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

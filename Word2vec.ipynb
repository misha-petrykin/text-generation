{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import get_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(('fiction.lowercased.tokenized.word2vec.300d'),binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110363\n",
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.267473,  0.770113, -2.217801, -2.82495 ,  0.195229,  0.080929,\n",
       "        0.576751,  0.906972,  1.862315, -0.97951 ], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(model.vocab))\n",
    "print(len(model['i']))\n",
    "model['i'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43079169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.16205267e-027,  2.03534858e+180,  9.03296881e+271, ...,\n",
       "       -1.65833817e-303, -6.52529546e-295,  7.29293688e-304])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr=np.fromfile('uk_model.dat')\n",
    "print(len(arr))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131827.85333333333"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.fromfile('fiction.lowercased.tokenized.word2vec.300d'))/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
    "path = get_file('arxiv_abstracts.txt', origin=url)\n",
    "\n",
    "max_sentence_len = 40\n",
    "with open(path) as file_:\n",
    "    docs = file_.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fiction.tokenized.shuffled.txt') as f:\n",
    "    docs = f.readlines()\n",
    "l=list()\n",
    "for i in docs:\n",
    "    if i!='\\n' and i!='* * *\\n':\n",
    "        l.append(i)\n",
    "max_sentence_len = 10\n",
    "docs=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open('uk_model.dat') as f:\n",
    "# #     d = f.readlines()\n",
    "# d=np.fromfile('uk_model.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 1811548\n"
     ]
    }
   ],
   "source": [
    "sentences = [[word for word in doc.lower().translate(string.punctuation).split()[:max_sentence_len]] for doc in docs]\n",
    "print('Num sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "word_model.wv.vocab.keys() #watch words in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (689, 20)\n",
      "Checking similar words:\n",
      "  пришло -> потому (1.00), штука (1.00), чтобы (1.00), мы (1.00), девица (1.00), повернулся (1.00), он (1.00), решил (1.00)\n",
      "  латинист -> пытку. (1.00), раз (1.00), твоего (1.00), с (1.00), бы (1.00), воскликнул (1.00), любого (1.00), увернулся (1.00)\n",
      "  штука -> девица (1.00), от (1.00), я, (1.00), получится (1.00), браннер (1.00), казалось, (1.00), взглядом (1.00), — (1.00)\n",
      "  глядела -> пошел (0.88), собачился (0.88), назад, (0.88), толкнул!+ (0.88), понимал, (0.88), тряслись, (0.88), дураки, (0.88), перешептывались.+ (0.88)\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = word_model.wv.vectors\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "# for word in ['model', 'network', 'train', 'learn']:\n",
    "for word in ['пришло', 'латинист', 'штука', 'глядела']:\n",
    "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n",
    "    print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return word_model.wv.vocab[word].index #get word by index:)\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index2word[idx] #get index by word:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the data for LSTM...\n",
      "train_x shape: (146, 10)\n",
      "train_y shape: (146,)\n"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the data for LSTM...')\n",
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        train_x[i, t] = word2idx(word)\n",
    "        train_y[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['послушай,', 'я', 'не', 'хотел', 'быть', 'полукровкой.+']\n",
      "послушай,\n",
      "я\n",
      "не\n",
      "хотел\n",
      "быть\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "for t, word in enumerate(sentences[0][:-1]):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "for\n",
      "restarting\n",
      "almost\n",
      "rate\n",
      "batch-processing\n",
      "as\n",
      "and\n",
      "achievable\n",
      "coordinates\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(train_y):\n",
    "    if i==10:\n",
    "        break\n",
    "    print(idx2word(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 146 samples\n",
      "Epoch 1/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.5098\n",
      "Generating text after epoch: 0\n",
      "она... -> она раз не-а, убийственно, заткнешься?+ тебя остановился упс! воскликнул понял, умри,\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 6.5073\n",
      "Epoch 2/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.4952\n",
      "Generating text after epoch: 1\n",
      "она... -> она отозвался все, расселся мне пронзительно экскурсиях спокойствие. хоть снаружи швырялись\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 5ms/sample - loss: 6.4930\n",
      "Epoch 3/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.4761\n",
      "Generating text after epoch: 2\n",
      "она... -> она доддз повторял авеню, просто вопрос, понятно. отозвался большинство самодовольно тем\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 6.4777\n",
      "Epoch 4/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.4600\n",
      "Generating text after epoch: 3\n",
      "она... -> она нэнси легкой ланча проведения ему взять дойдя умри, молча. чувствительный.+\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 5ms/sample - loss: 6.4607\n",
      "Epoch 5/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.4404\n",
      "Generating text after epoch: 4\n",
      "она... -> она застревавшие сэндвич, я.+ развернуть случилось ланч когда среднего кронос куртку\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 7ms/sample - loss: 6.4420\n",
      "Epoch 6/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.4207\n",
      "Generating text after epoch: 5\n",
      "она... -> она ватные. какое слышал?+ имел, ты! …ее остаться что состоял ехавших\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 5ms/sample - loss: 6.4210\n",
      "Epoch 7/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.3965\n",
      "Generating text after epoch: 6\n",
      "она... -> она дураки, мне, легкой моей знаю, проблемы, чему послышались частную взять\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 6ms/sample - loss: 6.3972\n",
      "Epoch 8/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.3721\n",
      "Generating text after epoch: 7\n",
      "она... -> она мелкой взглядом доддз смертоубийственным получится всю каштановых дорогуша, знать остальными.+\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 6.3699\n",
      "Epoch 9/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.3386\n",
      "Generating text after epoch: 8\n",
      "она... -> она экскурсий ухмыльнулась, полукровкой.+ ошибался.+ крекерами. подальше вынужден колонны пронеслась ты\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 6.3391\n",
      "Epoch 10/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.3013\n",
      "Generating text after epoch: 9\n",
      "она... -> она начать швырялись «она своих аппетит пробормотал яблоко титанами, разочарованный. свою\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 7ms/sample - loss: 6.3043\n",
      "Epoch 11/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.2773\n",
      "Generating text after epoch: 10\n",
      "она... -> она послышались постараюсь, гроувер глубь сплошь сейчас думаю, убийственно, отчаянии шуточку\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 6ms/sample - loss: 6.2646\n",
      "Epoch 12/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.2226\n",
      "Generating text after epoch: 11\n",
      "она... -> она толкались сторону настойчиво понятия тем, этой — так находиться тут\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 6.2216\n",
      "Epoch 13/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.1740\n",
      "Generating text after epoch: 12\n",
      "она... -> она него. ярче, джорджии, отозвался говорить!+ тем, настоящую кто-то страницах, в\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 6.1740\n",
      "Epoch 14/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.1223\n",
      "Generating text after epoch: 13\n",
      "она... -> она тем, путем, рассчитывал.+ плечами.+ любого коляске фирменным самодовольно смертоубийственным придется\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 6.1231\n",
      "Epoch 15/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.0724\n",
      "Generating text after epoch: 14\n",
      "она... -> она мере пришло перси! остальными.+ взглядом частную раз, постойте! экскурсий божеством?\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 8ms/sample - loss: 6.0692\n",
      "Epoch 16/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 6.0136\n",
      "Generating text after epoch: 15\n",
      "она... -> она называемых читать случаются глубь задавала здание заявила математики всю дело.\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 6ms/sample - loss: 6.0134\n",
      "Epoch 17/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 5.9651\n",
      "Generating text after epoch: 16\n",
      "она... -> она жизнь. исчезла. поводу экскурсий большим и неважный, галерее мистер сказал,\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 5.9563\n",
      "Epoch 18/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 5.8972\n",
      "Generating text after epoch: 17\n",
      "она... -> она шел ну… прямо любого более сверля сторону бы ну… видел?..+\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 5.8991\n",
      "Epoch 19/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 5.8533\n",
      "Generating text after epoch: 18\n",
      "она... -> она будто кто-то перешептывались.+ школьный что из-за истекло, такой, у решил,\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 4ms/sample - loss: 5.8420\n",
      "Epoch 20/20\n",
      "128/146 [=========================>....] - ETA: 0s - loss: 5.7890\n",
      "Generating text after epoch: 19\n",
      "она... -> она прерваться.+ о'кей. глядела после дорогуша…+ за вопросы дружище, так.+ понятия\n",
      "--------------------------------------------------\n",
      "\n",
      "146/146 [==============================] - 1s 6ms/sample - loss: 5.7867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fce2c0caa50>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=10):\n",
    "    word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "    for i in range(num_generated):\n",
    "        prediction = model.predict(x=np.array(word_idxs))\n",
    "        idx = sample(prediction[-1], temperature=0.7)\n",
    "        word_idxs.append(idx)\n",
    "    return ' '.join(idx2word(idx) for idx in word_idxs)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print('\\nGenerating text after epoch: %d' % epoch)\n",
    "    texts = [\n",
    "#     'deep convolutional',\n",
    "        \n",
    "        \n",
    "#     'simple and effective',\n",
    "#     'a nonconvex',\n",
    "#     'a'\n",
    "    'она'\n",
    "    ]\n",
    "    break_s='\\n'+'-'*50+'\\n'\n",
    "    for text in texts:\n",
    "        sample = generate_next(text)\n",
    "        print('%s... -> %s%s' % (text, sample,break_s))\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

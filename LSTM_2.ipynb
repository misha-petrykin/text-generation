{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching the text...\n",
      "\n",
      "Preparing the sentences...\n",
      "Num sentences: 7200\n",
      "\n",
      "Training word2vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "/home/misha/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (1351, 100)\n",
      "Checking similar words:\n",
      "  model -> technique (0.36), $l_p$ (0.36), architecture. (0.34), extend (0.34), trains (0.33), approach (0.32), of (0.30), rnn, (0.29)\n",
      "  network -> networks (0.41), constrained (0.24), architecture (0.24), given (0.23), trained (0.23), there (0.21), help (0.21), accepted (0.21)\n",
      "  train -> based (0.42), eigendecompositions (0.34), classical (0.33), improve (0.31), construct (0.30), then (0.29), average (0.29), extend (0.28)\n",
      "  learn -> automatically (0.40), tend (0.37), adopted (0.37), remain (0.35), effectively (0.35), adapt (0.35), relevant (0.34), lower (0.34)\n",
      "\n",
      "Preparing the data for LSTM...\n",
      "train_x shape: (7200, 40)\n",
      "train_y shape: (7200,)\n",
      "\n",
      "Training LSTM...\n",
      "Train on 7200 samples\n",
      "Epoch 1/2\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 4.4191\n",
      "Generating text after epoch: 0\n",
      "deep convolutional... -> deep convolutional (dbn) now discuss back-propagated datasets (rnn) distributions. labels theoretical experimentally,\n",
      "--------------------------------------------------\n",
      "\n",
      "simple and effective... -> simple and effective realize foundation systems studied dimensionality input $d$-dimensional arguing bound restricted\n",
      "--------------------------------------------------\n",
      "\n",
      "a nonconvex... -> a nonconvex series address loss sequentially despite best level caused this making\n",
      "--------------------------------------------------\n",
      "\n",
      "a... -> a mass distribution precision dnns, configuration sparse, locally, boost estimate clusterings,\n",
      "--------------------------------------------------\n",
      "\n",
      "7200/7200 [==============================] - 35s 5ms/sample - loss: 4.4076\n",
      "Epoch 2/2\n",
      "7168/7200 [============================>.] - ETA: 0s - loss: 0.9974\n",
      "Generating text after epoch: 1\n",
      "deep convolutional... -> deep convolutional depth, presented pretraining policies available, combine error they high digit\n",
      "--------------------------------------------------\n",
      "\n",
      "simple and effective... -> simple and effective capacity unit, structure by that performed many important hierarchy. net\n",
      "--------------------------------------------------\n",
      "\n",
      "a nonconvex... -> a nonconvex further domain leverages a regularizing namely, (input language improved a\n",
      "--------------------------------------------------\n",
      "\n",
      "a... -> a therefore, architecture over neuron. choice neural unsupervised (anrat), thereby machinery\n",
      "--------------------------------------------------\n",
      "\n",
      "7200/7200 [==============================] - 26s 4ms/sample - loss: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efdb2243350>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "print('\\nFetching the text...')\n",
    "url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
    "path = get_file('arxiv_abstracts.txt', origin=url)\n",
    "\n",
    "print('\\nPreparing the sentences...')\n",
    "max_sentence_len = 40\n",
    "with open(path) as file_:\n",
    "    docs = file_.readlines()\n",
    "# file = open(\"long.txt\").read()\n",
    "# docs=file.split('\\n')\n",
    "# docs=list(file)\n",
    "\n",
    "sentences = [[word for word in doc.lower().translate(string.punctuation).split()[:max_sentence_len]] for doc in docs]\n",
    "print('Num sentences:', len(sentences))\n",
    "\n",
    "print('\\nTraining word2vec...')\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['model', 'network', 'train', 'learn']:\n",
    "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
    "    print('  %s -> %s' % (word, most_similar))\n",
    "\n",
    "def word2idx(word):\n",
    "    return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index2word[idx]\n",
    "\n",
    "print('\\nPreparing the data for LSTM...')\n",
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        train_x[i, t] = word2idx(word)\n",
    "        train_y[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)\n",
    "\n",
    "print('\\nTraining LSTM...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=10):\n",
    "    word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "    for i in range(num_generated):\n",
    "        prediction = model.predict(x=np.array(word_idxs))\n",
    "        idx = sample(prediction[-1], temperature=0.7)\n",
    "        word_idxs.append(idx)\n",
    "    return ' '.join(idx2word(idx) for idx in word_idxs)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print('\\nGenerating text after epoch: %d' % epoch)\n",
    "    texts = [\n",
    "    'deep convolutional',\n",
    "    'simple and effective',\n",
    "    'a nonconvex',\n",
    "    'a'\n",
    "    ]\n",
    "    break_s='\\n'+'-'*50+'\\n'\n",
    "    for text in texts:\n",
    "        sample = generate_next(text)\n",
    "        print('%s... -> %s%s' % (text, sample,break_s))\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=model.predict(train_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
